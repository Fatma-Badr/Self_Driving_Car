def src_bottom_points(img):
    mid_width=(int)((img.shape[1])/2)
    print(mid_width)
    mid_height=(int)((img.shape[0])/2)
    height=(int)(img.shape[0])
    width=(int)(img.shape[1])
    #src bottom left
    b=False
    for j in range(height-1,0,-1):
        for i in range(mid_width,0,-1):            
#             print(j,i)
            if img[j][i]==255:
               b=True
               break
        if b==True:
            break
    
    #src bottom right
    m=False
    for k in range(height-1,0,-1):
        for l in range(mid_width,width,1):            
#                print(k,l)
            if img[k][l]==255:                
                m=True
                break
        
        if m==True:
            break
        
                
        
                
    return j,i,k,l        
 def distance(img):
    #convert image to HSL color space then use s channel
    hsl=cv2.cvtColor(img,cv2.COLOR_RGB2HLS)    
    s=hsl[:,:,2]
    # apply canny to s channel
    edge_image=cv2.Canny(s,200,100)
    #calling a function to find the bottom source points 
    lv,lh,rv,rh=src_bottom_points(edge_image)
    center_of_the_lane=(rh-lh)/2
    print(center_of_the_lane)
    center_of_the_car=(img.shape[1])/2
    print(center_of_the_car)
    s="vehicle is "
    if (center_of_the_car-center_of_the_lane)>0:
        dist=(int)((center_of_the_car-center_of_the_lane)* 0.0002645833*1000)
        s+=str(dist/1000)+"m right of center"
    else:
        dist=(int)(np.absolute((center_of_the_car-center_of_the_lane))* 0.0002645833*1000)
        s+=str(dist/1000)+"m left of center"
    print(s)    
    return s   


def pipeline (img):
  #convert the image read to RGB
  image=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
  #convert the image to HLS to get rid of shadows
  hls_image = cv2.cvtColor(image, cv2.COLOR_RGB2HLS) 
  h_channel = hls_image[:, :, 0]
  l_channel = hls_image[:, :, 1]
  s_channel = hls_image[:, :, 2]
  #using the sobel filter to get the edges
  sobelx = cv2.Sobel(l_channel, cv2.CV_64F, 1, 0)      
  abs_sobel_x = np.absolute(sobelx)
  #scaling the edges image to a range from 0 to 255
  scaled_sobel = (255 * ((abs_sobel_x) / np.max(abs_sobel_x))).astype(np.uint8)
  #thresholding the image to inhance the lane edges
  sx_thresh = [10, 100]
  sx_binary = (np.zeros_like(scaled_sobel)).astype(np.uint8)             
  sx_binary[(scaled_sobel > sx_thresh[0]) & (scaled_sobel < sx_thresh[1])] = 1
  #creating the bird prespective to highlight the lanes
  srcPoints = np.array([[300, 670],
                    [550, 500],
                    [780, 500],
                    [1050, 670]]).astype(np.float32)

  dstPoints = np.array([[200, 700],
                    [200, 50],
                    [1000, 50],
                    [1000, 700]]).astype(np.float32)

  M, Minv = perspectiveTransform(srcPoints, dstPoints)

  warped_image = (warpPerspective(sx_binary, sx_binary.astype(np.float32).shape[1::-1], M)).astype(np.uint8)  #
  cont = np.array([[200, 700],
                    [200, 50],
                    [1000, 50],
                    [1000, 700]])
  cv2.fillPoly(warped_image, pts = [cont], color =(255,0,0))
  original_image = warpPerspective(warped_image, sx_binary.shape[1::-1], Minv).astype(np.uint8) #
  b,g,r = cv2.split(img)
  newArr = np.zeros((720, 1280)).astype(np.uint8)
  for i in range(len(original_image)):
      for j in range(len(original_image[i])):
          if original_image[i][j]>=b[i][j]:
              newArr[i][j]=original_image[i][j]
          else:
              newArr[i][j]=b[i][j]
  //tranforming the image back to the original prespective
  output = (cv2.merge((r,g,newArr))).astype(np.uint8)
  doutput=distance(image)
  # vis = np.concatenate((hls_image,output), axis=0)
  # print("hls",hls_image.shape,"s",s_channel.shape,"sobel",scaled_sobel.shape,"sx_bin",sx_binary.shape,"warped",warped_image.shape,"original",original_image.shape)
  outputf=cv2.putText(img=output, text=doutput, org=(50, 150), fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=1, color=(0,0, 255),thickness=1)
  return [hls_image,s_channel ,scaled_sobel,original_image,outputf]
  
  
  #read and write video
  import sys

argv = sys.argv
path_in = argv[1]
debug = int(argv[2])
path_out = argv[3]
print(path_in)
print(debug)
print(path_out)

#collect frames here
col_images=[]


# Create a VideoCapture object and read from input file
cap = cv2.VideoCapture(path_in)
fps = cap.get(cv2.CAP_PROP_FPS)
print(fps)
print("hi")

 

# Check if camera opened successfully
if (cap.isOpened()== False):
    print("Error opening video file")

 

frame_width = int(cap.get(3))
frame_height = int(cap.get(4))
#out is used for the main detection video
#out2 is used for the different pipeline stages video
out = cv2.VideoWriter(path_out,cv2.VideoWriter_fourcc('M','J','P','G'),fps, (frame_width,frame_height))
out2 = cv2.VideoWriter(path_out,cv2.VideoWriter_fourcc('M','J','P','G'),1, (frame_width,frame_height))

 

while(cap.isOpened()):
    
    #read the video frame by frame
    # Capture frame-by-frame
    
    ret, frame = cap.read()
    if ret == True:
        col_images.append(frame)
        frame_out = pipeline (frame)
        #output 
        out.write(cv2.cvtColor(frame_out[4], cv2.COLOR_BGR2RGB))
        out2.write(frame_out[0])
        out2.write(cv2.cvtColor(frame_out[1], cv2.COLOR_GRAY2RGB))
        out2.write(cv2.cvtColor(frame_out[2], cv2.COLOR_GRAY2RGB))
        out2.write(cv2.cvtColor(frame_out[3], cv2.COLOR_GRAY2RGB))
        out2.write(cv2.cvtColor(frame_out[4], cv2.COLOR_BGR2RGB))
        # Press Q on keyboard to exit
        if cv2.waitKey(25) & 0xFF == ord('q'):
            break

    # Break the loop
    else:
        break

# When everything done, release
# the video capture object
cap.release()
out.release()

# Closes all the frames
cv2.destroyAllWindows()
